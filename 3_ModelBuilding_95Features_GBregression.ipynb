{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "//anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# This notebook takes output from Train_Common_FeatureSelectionWithLasso\n",
    "# Purpose: Use gradient boosting regression algorithm to train \n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "#Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor  #GBM algorithm\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import training data set\n",
    "Training_dataset = pd.read_csv('Training_95_Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4209, 95)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect data set\n",
    "Training_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X181</th>\n",
       "      <th>X119</th>\n",
       "      <th>X47</th>\n",
       "      <th>X156</th>\n",
       "      <th>X151</th>\n",
       "      <th>X324</th>\n",
       "      <th>X321</th>\n",
       "      <th>X12</th>\n",
       "      <th>X84</th>\n",
       "      <th>X218</th>\n",
       "      <th>...</th>\n",
       "      <th>X5_w</th>\n",
       "      <th>X5_r</th>\n",
       "      <th>X5_s</th>\n",
       "      <th>X5_l</th>\n",
       "      <th>X0_aj</th>\n",
       "      <th>X0_d</th>\n",
       "      <th>X0_h</th>\n",
       "      <th>X0_o</th>\n",
       "      <th>X0_t</th>\n",
       "      <th>X8_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   X181  X119  X47  X156  X151  X324  X321  X12  X84  X218  ...   X5_w  X5_r  \\\n",
       "0     0     1    0     1     0     1     0    0    0     0  ...      0     0   \n",
       "1     0     1    0     1     0     0     0    0    0     0  ...      0     0   \n",
       "2     0     0    0     0     0     1     0    0    1     1  ...      0     0   \n",
       "3     0     0    0     0     0     0     0    0    1     1  ...      0     0   \n",
       "4     0     0    0     0     0     0     0    0    0     1  ...      0     0   \n",
       "\n",
       "   X5_s  X5_l  X0_aj  X0_d  X0_h  X0_o  X0_t  X8_x  \n",
       "0     0     0      0     0     0     0     0     0  \n",
       "1     0     0      0     0     0     0     0     0  \n",
       "2     0     0      0     0     0     0     0     1  \n",
       "3     0     0      0     0     0     0     0     0  \n",
       "4     0     0      0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import target data \n",
    "Target = pd.read_csv('Target_Variable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert data set into array\n",
    "Training_array = Training_dataset.values\n",
    "# get the features\n",
    "Predictors = Training_array[:]\n",
    "# Convert target into array\n",
    "Target_array = Target.values\n",
    "# Get the Target \n",
    "Targets = Target_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 130.81,   88.53,   76.26, ...,  109.22,   87.48,  110.85])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, target, predictors, performCV=True, printFeatureImportance=True, cv_folds=cv):\n",
    "    \n",
    "    #Fit the algorithm on the training data\n",
    "    alg.fit(predictors,target)\n",
    "        \n",
    "    #Use the model fitted to predict training set:\n",
    "    dtrain_predictions = alg.predict(predictors)\n",
    "    \n",
    "    # Evaluate the predictions made by this fit\n",
    "    # The quality control I chose for my fit include mean squared error and R_squared\n",
    "    MSE=metrics.mean_squared_error(target,dtrain_predictions)\n",
    "    R_squared = metrics.r2_score(target,dtrain_predictions)\n",
    "    \n",
    "    \n",
    "    # The above code is without any cross validation where all the data is used to train the model.\n",
    "    # Well we know that leaves us open to overfitting. So to mitigate that, lets do some cross validation\n",
    "    # Also we will use this CV to tune some of our parameters \n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_scoreMSE = cross_validation.cross_val_score(alg, predictors,target, cv=cv_folds, scoring='neg_mean_squared_error')\n",
    "        cv_scoreR_squared = cross_validation.cross_val_score(alg, predictors,target, cv=cv_folds, scoring='r2')\n",
    "    \n",
    "    #Print model report:\n",
    "    print \"\\nModel Report\"\n",
    "    print \"MSE : %.4g\" % MSE\n",
    "    print \"R_squared : %.6g\" % R_squared\n",
    "    \n",
    "    \n",
    "    if performCV:\n",
    "        print \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_scoreMSE),np.std(cv_scoreMSE),np.min(cv_scoreMSE),np.max(cv_scoreMSE))\n",
    "        print \"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_scoreR_squared),np.std(cv_scoreR_squared),np.min(cv_scoreR_squared),np.max(cv_scoreR_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have this function that will allow us to fit a gradient boosting regression on a data set\n",
    "# get a model and then use the model to make predictions, evaluate the quality of these predictions\n",
    "# by way of mean squared error means and R squared.\n",
    "\n",
    "# First thing, lets fit a model with mostly default parameter settings and set a benchmark for \n",
    "# our model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 59.66\n",
      "R_squared : 0.628842\n",
      "CV Score : Mean - -71.43274 | Std - 33.75996 | Min - -162.8597 | Max - -42.34306\n",
      "CV Score : Mean - 0.5710286 | Std - 0.1050077 | Min - 0.309568 | Max - 0.6801093\n"
     ]
    }
   ],
   "source": [
    "# Now time to fit the first model called gbm0\n",
    "cv = 5\n",
    "gbm0 = GradientBoostingRegressor(random_state=10)\n",
    "# Now fit it on the training data set with default settings and a CV of 5\n",
    "modelfit(gbm0,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 59.66\n",
      "R_squared : 0.628842\n",
      "CV Score : Mean - -71.43274 | Std - 33.75996 | Min - -162.8597 | Max - -42.34306\n",
      "CV Score : Mean - 0.5710286 | Std - 0.1050077 | Min - 0.309568 | Max - 0.6801093\n"
     ]
    }
   ],
   "source": [
    "# That was cv of 5, lets do cv of 10\n",
    "cv = 10\n",
    "modelfit(gbm0,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Base model all default settings gave an R squared of 0.62 with the poorest at 0.3\n",
    "\n",
    "# I wonder how this will do in the kaggle board. I have already embarrassed myself with a R score of 0.1\n",
    "# So I figured with that, having shame should no longer be a concern of mine hahaha\n",
    "# So just predict and post the predictions of the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving this as base model to be used to fit on the Test model\n",
    "BaseModel = gbm0.fit(Predictors, Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  78.33632892,   94.40492763,   78.93267331, ...,   92.41118851,\n",
       "        109.87762172,   92.39030036])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load \n",
    "Test_95_Features = pd.read_csv('Test_95_Features.csv')\n",
    "Test_array =Test_95_Features.values\n",
    "Prediction = BaseModel.predict(Test_array)\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on this model, these are my predictions.\n",
    "\n",
    "# Save that in an csv file as Predictions_95FeaturesBaseModel\n",
    "np.savetxt('Predictions_95FeaturesBaseModel.csv',Prediction, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Whoaaaa...my score went from an embarrasingly 0.1 to 0.5378. Note I didnt do anything ground breaking here\n",
    "# I just made sure that the features I was using for the train was the same as test. I definitely didnt do that last time\n",
    "# which lead to just garbage model as I was basically using a model from a train data on a test data that was different\n",
    "\n",
    "# The 0.5378 still left me at number 2535 position. So again, I just did the default thingy right\n",
    "# But still I tweeted this LOL...can you believe this? Shameless plug at its finest! Hahahahah\n",
    "\n",
    "# But now the question is...can we improve this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One of the major parameters to tune is the number of trees that are used\n",
    "# There is no default optimum trees, so we have to tune the number of trees using the parameter n_estimators\n",
    "\n",
    "# Lets test the number of trees from a range of 20 to 80 in steps of 10\n",
    "param_test1 ={'n_estimators':range(20,81,10)}\n",
    "\n",
    "# Lets set some parameters\n",
    "# We choose a learning rate of 0.1\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10),param_grid = param_test1, scoring='r2',n_jobs=4,iid=False, cv=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=8,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=50,\n",
       "             min_samples_split=500, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=10,\n",
       "             subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'n_estimators': [20, 30, 40, 50, 60, 70, 80]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.54590, std: 0.06708, params: {'n_estimators': 20},\n",
       "  mean: 0.56224, std: 0.07017, params: {'n_estimators': 30},\n",
       "  mean: 0.56638, std: 0.07131, params: {'n_estimators': 40},\n",
       "  mean: 0.56741, std: 0.07201, params: {'n_estimators': 50},\n",
       "  mean: 0.56799, std: 0.07261, params: {'n_estimators': 60},\n",
       "  mean: 0.56774, std: 0.07243, params: {'n_estimators': 70},\n",
       "  mean: 0.56731, std: 0.07231, params: {'n_estimators': 80}],\n",
       " {'n_estimators': 60},\n",
       " 0.5679893790171912)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So as we can see here, the optimal number of trees built here is 60\n",
    "# So we have the number of trees as 60. Now lets tune the tree parameters and the parameters we want to tune\n",
    "# are :\n",
    "# 1) Tune max_depth and num_samples_split\n",
    "# 2) Tune min_samples_leaf\n",
    "# 3) Tune max_features\n",
    "\n",
    "# max_depth and min_samples_split have a significant impact and we’re tuning those first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To start with, I’ll test max_depth values of 5 to 15 in steps of 2 and min_samples_split \n",
    "# from 200 to 1000 in steps of 200.\n",
    "\n",
    "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test2, scoring='r2',n_jobs=4,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=60, presort='auto', random_state=10,\n",
       "             subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'min_samples_split': [200, 400, 600, 800, 1000], 'max_depth': [5, 7, 9, 11, 13, 15]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57183, std: 0.07347, params: {'min_samples_split': 200, 'max_depth': 5},\n",
       "  mean: 0.57226, std: 0.07402, params: {'min_samples_split': 400, 'max_depth': 5},\n",
       "  mean: 0.57332, std: 0.07370, params: {'min_samples_split': 600, 'max_depth': 5},\n",
       "  mean: 0.57262, std: 0.07280, params: {'min_samples_split': 800, 'max_depth': 5},\n",
       "  mean: 0.57413, std: 0.07150, params: {'min_samples_split': 1000, 'max_depth': 5},\n",
       "  mean: 0.57178, std: 0.07738, params: {'min_samples_split': 200, 'max_depth': 7},\n",
       "  mean: 0.57220, std: 0.07552, params: {'min_samples_split': 400, 'max_depth': 7},\n",
       "  mean: 0.57486, std: 0.07530, params: {'min_samples_split': 600, 'max_depth': 7},\n",
       "  mean: 0.57514, std: 0.07441, params: {'min_samples_split': 800, 'max_depth': 7},\n",
       "  mean: 0.57378, std: 0.07260, params: {'min_samples_split': 1000, 'max_depth': 7},\n",
       "  mean: 0.56630, std: 0.07398, params: {'min_samples_split': 200, 'max_depth': 9},\n",
       "  mean: 0.57312, std: 0.07516, params: {'min_samples_split': 400, 'max_depth': 9},\n",
       "  mean: 0.57293, std: 0.07562, params: {'min_samples_split': 600, 'max_depth': 9},\n",
       "  mean: 0.57415, std: 0.07566, params: {'min_samples_split': 800, 'max_depth': 9},\n",
       "  mean: 0.57390, std: 0.07323, params: {'min_samples_split': 1000, 'max_depth': 9},\n",
       "  mean: 0.56494, std: 0.07442, params: {'min_samples_split': 200, 'max_depth': 11},\n",
       "  mean: 0.57302, std: 0.07521, params: {'min_samples_split': 400, 'max_depth': 11},\n",
       "  mean: 0.57402, std: 0.07618, params: {'min_samples_split': 600, 'max_depth': 11},\n",
       "  mean: 0.57450, std: 0.07534, params: {'min_samples_split': 800, 'max_depth': 11},\n",
       "  mean: 0.57398, std: 0.07382, params: {'min_samples_split': 1000, 'max_depth': 11},\n",
       "  mean: 0.56824, std: 0.07448, params: {'min_samples_split': 200, 'max_depth': 13},\n",
       "  mean: 0.57210, std: 0.07620, params: {'min_samples_split': 400, 'max_depth': 13},\n",
       "  mean: 0.57292, std: 0.07629, params: {'min_samples_split': 600, 'max_depth': 13},\n",
       "  mean: 0.57485, std: 0.07571, params: {'min_samples_split': 800, 'max_depth': 13},\n",
       "  mean: 0.57456, std: 0.07403, params: {'min_samples_split': 1000, 'max_depth': 13},\n",
       "  mean: 0.56506, std: 0.07483, params: {'min_samples_split': 200, 'max_depth': 15},\n",
       "  mean: 0.57189, std: 0.07653, params: {'min_samples_split': 400, 'max_depth': 15},\n",
       "  mean: 0.57455, std: 0.07597, params: {'min_samples_split': 600, 'max_depth': 15},\n",
       "  mean: 0.57460, std: 0.07538, params: {'min_samples_split': 800, 'max_depth': 15},\n",
       "  mean: 0.57439, std: 0.07393, params: {'min_samples_split': 1000, 'max_depth': 15}],\n",
       " {'max_depth': 7, 'min_samples_split': 800},\n",
       " 0.5751406188517167)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The ideal max depth is 7 and 800 min_samples_split. Lets keep the max depth as 7\n",
    "# But lets tinker around with the min_samples_split and also tune min_samples_leaf\n",
    "\n",
    "# Here samples_split ranging from 800 to 2000 in steps of 200\n",
    "# While min_samples_leaf is between 30 to 71 in steps of 10\n",
    "param_test3 = {'min_samples_split':range(800,2000,200), 'min_samples_leaf':range(30,71,10)}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,max_depth=7, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test3, scoring='r2',n_jobs=4,iid=False, cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=60, presort='auto', random_state=10,\n",
       "             subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'min_samples_split': [800, 1000, 1200, 1400, 1600, 1800], 'min_samples_leaf': [30, 40, 50, 60, 70]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57280, std: 0.07389, params: {'min_samples_split': 800, 'min_samples_leaf': 30},\n",
       "  mean: 0.57317, std: 0.07245, params: {'min_samples_split': 1000, 'min_samples_leaf': 30},\n",
       "  mean: 0.57169, std: 0.07153, params: {'min_samples_split': 1200, 'min_samples_leaf': 30},\n",
       "  mean: 0.57043, std: 0.07158, params: {'min_samples_split': 1400, 'min_samples_leaf': 30},\n",
       "  mean: 0.56788, std: 0.07265, params: {'min_samples_split': 1600, 'min_samples_leaf': 30},\n",
       "  mean: 0.56283, std: 0.07103, params: {'min_samples_split': 1800, 'min_samples_leaf': 30},\n",
       "  mean: 0.56825, std: 0.07363, params: {'min_samples_split': 800, 'min_samples_leaf': 40},\n",
       "  mean: 0.56830, std: 0.07247, params: {'min_samples_split': 1000, 'min_samples_leaf': 40},\n",
       "  mean: 0.56804, std: 0.07183, params: {'min_samples_split': 1200, 'min_samples_leaf': 40},\n",
       "  mean: 0.56731, std: 0.07195, params: {'min_samples_split': 1400, 'min_samples_leaf': 40},\n",
       "  mean: 0.56421, std: 0.07209, params: {'min_samples_split': 1600, 'min_samples_leaf': 40},\n",
       "  mean: 0.56003, std: 0.07249, params: {'min_samples_split': 1800, 'min_samples_leaf': 40},\n",
       "  mean: 0.56757, std: 0.07343, params: {'min_samples_split': 800, 'min_samples_leaf': 50},\n",
       "  mean: 0.56765, std: 0.07221, params: {'min_samples_split': 1000, 'min_samples_leaf': 50},\n",
       "  mean: 0.56783, std: 0.07210, params: {'min_samples_split': 1200, 'min_samples_leaf': 50},\n",
       "  mean: 0.56682, std: 0.07180, params: {'min_samples_split': 1400, 'min_samples_leaf': 50},\n",
       "  mean: 0.56401, std: 0.07228, params: {'min_samples_split': 1600, 'min_samples_leaf': 50},\n",
       "  mean: 0.55870, std: 0.07145, params: {'min_samples_split': 1800, 'min_samples_leaf': 50},\n",
       "  mean: 0.56712, std: 0.07379, params: {'min_samples_split': 800, 'min_samples_leaf': 60},\n",
       "  mean: 0.56737, std: 0.07234, params: {'min_samples_split': 1000, 'min_samples_leaf': 60},\n",
       "  mean: 0.56642, std: 0.07188, params: {'min_samples_split': 1200, 'min_samples_leaf': 60},\n",
       "  mean: 0.56504, std: 0.07241, params: {'min_samples_split': 1400, 'min_samples_leaf': 60},\n",
       "  mean: 0.56357, std: 0.07298, params: {'min_samples_split': 1600, 'min_samples_leaf': 60},\n",
       "  mean: 0.55838, std: 0.07205, params: {'min_samples_split': 1800, 'min_samples_leaf': 60},\n",
       "  mean: 0.56649, std: 0.07277, params: {'min_samples_split': 800, 'min_samples_leaf': 70},\n",
       "  mean: 0.56668, std: 0.07191, params: {'min_samples_split': 1000, 'min_samples_leaf': 70},\n",
       "  mean: 0.56607, std: 0.07206, params: {'min_samples_split': 1200, 'min_samples_leaf': 70},\n",
       "  mean: 0.56410, std: 0.07166, params: {'min_samples_split': 1400, 'min_samples_leaf': 70},\n",
       "  mean: 0.56258, std: 0.07229, params: {'min_samples_split': 1600, 'min_samples_leaf': 70},\n",
       "  mean: 0.55540, std: 0.07131, params: {'min_samples_split': 1800, 'min_samples_leaf': 70}],\n",
       " {'min_samples_leaf': 30, 'min_samples_split': 1000},\n",
       " 0.5731702816491757)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hmmmm..after min_samples_leaf 30, the R2 score fell\n",
    "# So lets set between 1 to 30 min_samples_leaf\n",
    "param_test4 = {'min_samples_split':range(800,1200,200), 'min_samples_leaf':range(1,30,10)}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,max_depth=7, max_features='sqrt', subsample=0.8, random_state=10), \n",
    "param_grid = param_test4, scoring='r2',n_jobs=4,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=60, presort='auto', random_state=10,\n",
       "             subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'min_samples_split': [800, 1000], 'min_samples_leaf': [1, 11, 21]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch4.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57514, std: 0.07441, params: {'min_samples_split': 800, 'min_samples_leaf': 1},\n",
       "  mean: 0.57378, std: 0.07260, params: {'min_samples_split': 1000, 'min_samples_leaf': 1},\n",
       "  mean: 0.57551, std: 0.07405, params: {'min_samples_split': 800, 'min_samples_leaf': 11},\n",
       "  mean: 0.57408, std: 0.07275, params: {'min_samples_split': 1000, 'min_samples_leaf': 11},\n",
       "  mean: 0.57394, std: 0.07299, params: {'min_samples_split': 800, 'min_samples_leaf': 21},\n",
       "  mean: 0.57332, std: 0.07258, params: {'min_samples_split': 1000, 'min_samples_leaf': 21}],\n",
       " {'min_samples_leaf': 11, 'min_samples_split': 800},\n",
       " 0.5755101095563181)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So the optimum samples leaf is 11 or at least falls between 11 and 21\n",
    "# Lets test that range and see if we get any extra benefit. 11 and 21 with steps 1 every time\n",
    "param_test5 = {'min_samples_leaf':range(11,21,1)}\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,min_samples_split=800, max_features='sqrt',max_depth=7, subsample=0.8, random_state=10), \n",
    "param_grid = param_test5,scoring='r2',n_jobs=4,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=800, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=60, presort='auto', random_state=10,\n",
       "             subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'min_samples_leaf': [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch5.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57551, std: 0.07405, params: {'min_samples_leaf': 11},\n",
       "  mean: 0.57522, std: 0.07400, params: {'min_samples_leaf': 12},\n",
       "  mean: 0.57485, std: 0.07374, params: {'min_samples_leaf': 13},\n",
       "  mean: 0.57433, std: 0.07362, params: {'min_samples_leaf': 14},\n",
       "  mean: 0.57494, std: 0.07291, params: {'min_samples_leaf': 15},\n",
       "  mean: 0.57450, std: 0.07294, params: {'min_samples_leaf': 16},\n",
       "  mean: 0.57457, std: 0.07317, params: {'min_samples_leaf': 17},\n",
       "  mean: 0.57459, std: 0.07298, params: {'min_samples_leaf': 18},\n",
       "  mean: 0.57411, std: 0.07323, params: {'min_samples_leaf': 19},\n",
       "  mean: 0.57352, std: 0.07285, params: {'min_samples_leaf': 20}],\n",
       " {'min_samples_leaf': 11},\n",
       " 0.5755101095563181)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The min_samples_leaf returned 11. So lets test 1 to 11\n",
    "\n",
    "\n",
    "param_test6 = {'min_samples_leaf':range(1,11,1)}\n",
    "\n",
    "gsearch6 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,min_samples_split=800, max_features='sqrt',max_depth=7, subsample=0.8, random_state=10), \n",
    "param_grid = param_test6,scoring='r2',n_jobs=4,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=800, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=60, presort='auto', random_state=10,\n",
       "             subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch6.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57514, std: 0.07441, params: {'min_samples_leaf': 1},\n",
       "  mean: 0.57503, std: 0.07411, params: {'min_samples_leaf': 2},\n",
       "  mean: 0.57535, std: 0.07420, params: {'min_samples_leaf': 3},\n",
       "  mean: 0.57587, std: 0.07454, params: {'min_samples_leaf': 4},\n",
       "  mean: 0.57567, std: 0.07458, params: {'min_samples_leaf': 5},\n",
       "  mean: 0.57621, std: 0.07471, params: {'min_samples_leaf': 6},\n",
       "  mean: 0.57640, std: 0.07469, params: {'min_samples_leaf': 7},\n",
       "  mean: 0.57605, std: 0.07425, params: {'min_samples_leaf': 8},\n",
       "  mean: 0.57538, std: 0.07383, params: {'min_samples_leaf': 9},\n",
       "  mean: 0.57496, std: 0.07338, params: {'min_samples_leaf': 10}],\n",
       " {'min_samples_leaf': 7},\n",
       " 0.5763992576286825)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok..so lets take this for what it is now...\n",
    "# Tuning yielded optimum values for :\n",
    "# n_estimators = 60\n",
    "# max_depth': 7, \n",
    "# 'min_samples_split': 800\n",
    "# 'min_samples_leaf': 7\n",
    "\n",
    "\n",
    "# How about the max features. We have been using max features set as square root of features.\n",
    "# That sets it as max features as 10.\n",
    "# But lets tune that and see if we can get better leverage.\n",
    "\n",
    "# Here we set up a range of 10 to 20 and in steps of 2\n",
    "param_test7 = {'max_features':range(10,20,2)}\n",
    "\n",
    "gsearch7 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1,max_depth=7,min_samples_leaf=7, n_estimators=60,min_samples_split=800, subsample=0.8, random_state=10), \n",
    "param_grid = param_test7,scoring='r2',n_jobs=4,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=7, min_samples_split=800,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=60, presort='auto',\n",
       "             random_state=10, subsample=0.8, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'max_features': [10, 12, 14, 16, 18]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch7.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57399, std: 0.07408, params: {'max_features': 10},\n",
       "  mean: 0.57569, std: 0.07468, params: {'max_features': 12},\n",
       "  mean: 0.57524, std: 0.07355, params: {'max_features': 14},\n",
       "  mean: 0.57679, std: 0.07398, params: {'max_features': 16},\n",
       "  mean: 0.57650, std: 0.07326, params: {'max_features': 18}],\n",
       " {'max_features': 16},\n",
       " 0.576791407855946)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So now our max feature is 16..so lets summarize again:\n",
    "# n_estimators = 60\n",
    "# max_depth': 7, \n",
    "# 'min_samples_split': 800\n",
    "# 'min_samples_leaf': 7\n",
    "# 'max_features' : 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets continue tuning...\n",
    "# First subsample whose default is 0.8. Lets tune that\n",
    "param_test8 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
    "\n",
    "gsearch8 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1,max_depth=7,min_samples_leaf=7, n_estimators=60,min_samples_split=800,max_features=16, random_state=10), \n",
    "param_grid = param_test8,scoring='r2',n_jobs=4,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7, max_features=16,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=7, min_samples_split=800,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=60, presort='auto',\n",
       "             random_state=10, subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch8.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.57719, std: 0.07311, params: {'subsample': 0.6},\n",
       "  mean: 0.57675, std: 0.07307, params: {'subsample': 0.7},\n",
       "  mean: 0.57675, std: 0.07484, params: {'subsample': 0.75},\n",
       "  mean: 0.57679, std: 0.07398, params: {'subsample': 0.8},\n",
       "  mean: 0.57605, std: 0.07424, params: {'subsample': 0.85},\n",
       "  mean: 0.57641, std: 0.07336, params: {'subsample': 0.9}],\n",
       " {'subsample': 0.6},\n",
       " 0.5771948407410082)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch8.grid_scores_, gsearch8.best_params_, gsearch8.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hmm subsample started dropping at 0.6. So lets set 0.1 to 0.6\n",
    "\n",
    "# Lets continue tuning...\n",
    "# First subsample whose default is 0.8. Lets tune that\n",
    "param_test9 = {'subsample':[0.1,0.2,0.3,0.4,0.5,0.6]}\n",
    "\n",
    "gsearch9 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1,max_depth=7,min_samples_leaf=7, n_estimators=60,min_samples_split=800,max_features=16, random_state=10), \n",
    "param_grid = param_test9,scoring='r2',n_jobs=4,iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=7, max_features=16,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=7, min_samples_split=800,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=60, presort='auto',\n",
       "             random_state=10, subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=False, n_jobs=4,\n",
       "       param_grid={'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='r2', verbose=0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch9.fit(Predictors,Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: -0.00845, std: 0.01150, params: {'subsample': 0.1},\n",
       "  mean: -0.00790, std: 0.01007, params: {'subsample': 0.2},\n",
       "  mean: 0.56403, std: 0.07114, params: {'subsample': 0.3},\n",
       "  mean: 0.57010, std: 0.07077, params: {'subsample': 0.4},\n",
       "  mean: 0.57381, std: 0.07170, params: {'subsample': 0.5},\n",
       "  mean: 0.57719, std: 0.07311, params: {'subsample': 0.6}],\n",
       " {'subsample': 0.6},\n",
       " 0.5771948407410082)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch9.grid_scores_, gsearch9.best_params_, gsearch9.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So the sub sample is 0.6. Again summarize:\n",
    "# n_estimators = 60\n",
    "# max_depth': 7, \n",
    "# 'min_samples_split': 800\n",
    "# 'min_samples_leaf': 7\n",
    "# 'max_features' : 16\n",
    "# 'sub sample' : 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seems we have all our parameters tuned except the learning rate which we had set very high as 0.1.\n",
    "# Lets reduce that by half and see if it helps and when we do that, we can increase the number of estimates\n",
    "\n",
    "# So reduce the learning parameter by half and double the number of trees \n",
    "\n",
    "# So learning rate is 0.05 while n_estimators set as 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_tuned_1 = GradientBoostingRegressor(learning_rate=0.05, n_estimators=120,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 65.49\n",
      "R_squared : 0.59253\n",
      "CV Score : Mean - -69.13744 | Std - 30.18157 | Min - -149.5394 | Max - -41.73169\n",
      "CV Score : Mean - 0.5829006 | Std - 0.09051485 | Min - 0.3660387 | Max - 0.6847281\n"
     ]
    }
   ],
   "source": [
    "modelfit(gbm_tuned_1,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How about 0.01 and 700\n",
    "gbm_tuned_2 = GradientBoostingRegressor(learning_rate=0.01, n_estimators=700,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 64.99\n",
      "R_squared : 0.595638\n",
      "CV Score : Mean - -69.11224 | Std - 30.26539 | Min - -149.8495 | Max - -41.39238\n",
      "CV Score : Mean - 0.583141 | Std - 0.09083058 | Min - 0.364724 | Max - 0.6872915\n"
     ]
    }
   ],
   "source": [
    "modelfit(gbm_tuned_2,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I am not getting any extra leverage. Lets keep the estimate at 60 and then try at 0.1 and 0.05\n",
    "\n",
    "# 0.1\n",
    "gbm_tuned_3 = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 65.49\n",
      "R_squared : 0.592535\n",
      "CV Score : Mean - -69.34603 | Std - 30.32398 | Min - -150.3267 | Max - -41.68576\n",
      "CV Score : Mean - 0.5816968 | Std - 0.09068235 | Min - 0.362701 | Max - 0.6850751\n"
     ]
    }
   ],
   "source": [
    "modelfit(gbm_tuned_3,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.05\n",
    "gbm_tuned_4 = GradientBoostingRegressor(learning_rate=0.05, n_estimators=60,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 67.84\n",
      "R_squared : 0.577927\n",
      "CV Score : Mean - -70.17451 | Std - 29.98049 | Min - -150.0549 | Max - -42.58078\n",
      "CV Score : Mean - 0.5761008 | Std - 0.08835689 | Min - 0.3638534 | Max - 0.6783134\n"
     ]
    }
   ],
   "source": [
    "modelfit(gbm_tuned_4,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Well, 0.05 is not cool. Lets increase 0.1 and see if it helps. Try 0.2\n",
    "gbm_tuned_5 = GradientBoostingRegressor(learning_rate=0.2, n_estimators=60,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 63.53\n",
      "R_squared : 0.60475\n",
      "CV Score : Mean - -69.78469 | Std - 30.21935 | Min - -150.0892 | Max - -42.13692\n",
      "CV Score : Mean - 0.5787368 | Std - 0.09047795 | Min - 0.3637078 | Max - 0.6816666\n"
     ]
    }
   ],
   "source": [
    "modelfit(gbm_tuned_5,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seems to be higher for 0.2.\n",
    "\n",
    "# Now lets try 0.5\n",
    "gbm_tuned_6 = GradientBoostingRegressor(learning_rate=0.5, n_estimators=60,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 61.1\n",
      "R_squared : 0.619839\n",
      "CV Score : Mean - -73.03037 | Std - 31.03759 | Min - -157.3093 | Max - -44.9372\n",
      "CV Score : Mean - 0.558413 | Std - 0.08905005 | Min - 0.3330986 | Max - 0.6605113\n"
     ]
    }
   ],
   "source": [
    "modelfit(gbm_tuned_6,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets try learning rate as 1\n",
    "gbm_tuned_7 = GradientBoostingRegressor(learning_rate=1, n_estimators=60,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 62.84\n",
      "R_squared : 0.609034\n",
      "CV Score : Mean - -83.01067 | Std - 33.06961 | Min - -170.5176 | Max - -50.98866\n",
      "CV Score : Mean - 0.4963725 | Std - 0.09303524 | Min - 0.2771028 | Max - 0.6110393\n"
     ]
    }
   ],
   "source": [
    "modelfit(gbm_tuned_7,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  78.38385619,   93.31373963,   78.2496379 , ...,   91.28503451,\n",
       "        108.49637444,   86.45349914])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets use gbm_tuned_6 model for prediction\n",
    "# Saving this as base model to be used to fit on the Test model\n",
    "Model_tuned_6 = gbm_tuned_6.fit(Predictors, Targets)\n",
    "# Make predictions on our test data\n",
    "Prediction = Model_tuned_6.predict(Test_array)\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on this model, these are my predictions.\n",
    "\n",
    "# Save that in an csv file as Predictions_95FeaturesModel_tuned_6\n",
    "np.savetxt('Predictions_95FeaturesModel_tuned_6.csv',Prediction, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ok that sucked! After all that fancy tweaking all I did was reduce the quality of my predictions that I made\n",
    "# from the base model. My R2 squared on Kaggle went from 0.53728 to 0.53059. So basically my base model was better\n",
    "\n",
    "# I am going back to the base model....lets just tweak the learning rate in my base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 59.66\n",
      "R_squared : 0.628842\n",
      "CV Score : Mean - -71.43274 | Std - 33.75996 | Min - -162.8597 | Max - -42.34306\n",
      "CV Score : Mean - 0.5710286 | Std - 0.1050077 | Min - 0.309568 | Max - 0.6801093\n"
     ]
    }
   ],
   "source": [
    "# Now time to fit the first model called gbm0\n",
    "cv = 5\n",
    "gbm1 = GradientBoostingRegressor(random_state=10, learning_rate=0.1)\n",
    "# Now fit it on the training data set with default settings and a CV of 5\n",
    "modelfit(gbm1,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 42.73\n",
      "R_squared : 0.734142\n",
      "CV Score : Mean - -80.14048 | Std - 37.5326 | Min - -182.6821 | Max - -48.58629\n",
      "CV Score : Mean - 0.5180906 | Std - 0.1160001 | Min - 0.2255326 | Max - 0.6329434\n"
     ]
    }
   ],
   "source": [
    "# Returns back the defult model. Lets do 0.5\n",
    "# Now time to fit the first model called gbm0\n",
    "cv = 5\n",
    "gbm2 = GradientBoostingRegressor(random_state=10, learning_rate=0.5)\n",
    "# Now fit it on the training data set with default settings and a CV of 5\n",
    "modelfit(gbm2,Targets, Predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  77.538025  ,   95.04864951,   79.14371743, ...,   91.24428454,\n",
       "        110.26498932,   89.82739432])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wow that jumped really high. R square increased and MSE increased. Lets submit this to kaggle right away and see the effect\n",
    "# Lets use that for prediction\n",
    "\n",
    "# Saving this as base model to be used to fit on the Test model\n",
    "Model_gbm2 = gbm2.fit(Predictors, Targets)\n",
    "\n",
    "Prediction =Model_gbm2.predict(Test_array)\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on this model, these are my predictions.\n",
    "\n",
    "# Save that in an csv file as Predictions_95FeaturesModel_gbm2\n",
    "np.savetxt('Predictions_95FeaturesModel_gbm2.csv',Prediction, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE : 36.86\n",
      "R_squared : 0.77067\n",
      "CV Score : Mean - -92.74285 | Std - 41.70329 | Min - -206.2691 | Max - -55.68886\n",
      "CV Score : Mean - 0.4411005 | Std - 0.1257591 | Min - 0.1255373 | Max - 0.5751843\n"
     ]
    }
   ],
   "source": [
    "# Nope. Actually, it went from 0.5 to 0.4 . Damn!\n",
    "# Returns back the defult model. Lets do 0.5\n",
    "# Now time to fit the first model called gbm0\n",
    "cv = 5\n",
    "gbm3 = GradientBoostingRegressor(random_state=10, learning_rate=1)\n",
    "# Now fit it on the training data set with default settings and a CV of 5\n",
    "modelfit(gbm3,Targets, Predictors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_tuned_2 = GradientBoostingRegressor(learning_rate=0.01, n_estimators=700,max_depth=7, min_samples_split=800,min_samples_leaf=7, subsample=0.6, random_state=10, max_features=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  78.70074712,   93.88604202,   78.96082665, ...,   91.42385726,\n",
       "        110.33666902,   91.28708748])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_gbm_tuned_2 = gbm_tuned_2.fit(Predictors, Targets)\n",
    "Prediction =Model_gbm_tuned_2.predict(Test_array)\n",
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Based on this model, these are my predictions.\n",
    "\n",
    "# Save that in an csv file as Predictions_95FeaturesModel_gbm_tuned_2\n",
    "np.savetxt('Predictions_95FeaturesModel_gbm_tuned_2.csv',Prediction, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This approach actually helped. My score on the public leaderboard went from 0.53728 to 0.55226.\n",
    "\n",
    "# However, I was stuck and languishing at the 2423th position out of 3835 with the leading model having\n",
    "# a score of 0.67. But you know me, shameless...I tweeted this out! Hahahahah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I have a few rough ideas to help make my model better\n",
    "\n",
    "# 1) The 95 features that I have selected contains both categorical and continous data. One thing I can do\n",
    "# is to use PCA to summarize the continous data into three components and use those components instead of\n",
    "# of using the entire feature sets. Some might be correlated and thus adding to the noise\n",
    "\n",
    "# 2) I used the gradient boost regressor algorithm to generate this model. If I am really serious, I should use\n",
    "# the Xgboost algorithm. Some of the models that I built here clearly showed signs of over fitting since they \n",
    "# performed worse when I submitted my predictions on the test. Xgboost will give me the ability to apply some\n",
    "# form of regularisation(L1 or L2) on my models as I tweak my parameters. This might help.\n",
    "\n",
    "# 3) Also, I have not even explored the idea of a stacked model approach where I will build several models\n",
    "# and combine the models for prediction. I should plan on doing that.\n",
    "\n",
    "# 4) If everything fails, I will go back to using the entire features in the data set, build an Xgboost on all of them\n",
    "# and see if by just using regularisation if I can get a better model.\n",
    "\n",
    "# Bottom line: There are so many things that I am eager to try but experiments in the lab calling, abstracts to get\n",
    "# ready, presentations and posters to be made. So I will come back to this exciting data set when I have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update:\n",
    "\n",
    "# Well I never was able to squeeze out time and get back to this. The competition ended. But a little bit of the\n",
    "# cup not fully empty perspective: At the end of the compettition when 81 percent of the data was used to evaluate the\n",
    "# accuracy , my position improved from 2423 to 1590 th position with a score of 0.54796. The smart alec that won this \n",
    "# this had a score of 0.55551. \n",
    "\n",
    "# I will take time out and check out the strategies of some good models of this competition and see if some of the\n",
    "# above ideas were implemented to improve the models. I am also excited to see other creative and outside the box\n",
    "# thinking that these smart guys have implemented to get good models.\n",
    "\n",
    "# Exciting times ahead!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
